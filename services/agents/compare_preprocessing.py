"""
Comparison Script: Original vs Enhanced Preprocessing Results
This script compares the results of the original preprocessing with the enhanced version
"""

import pandas as pd
import numpy as np
import os

def load_and_compare_datasets():
    """Compare original and enhanced preprocessing results"""
    
    print("üîç PREPROCESSING COMPARISON ANALYSIS")
    print("=" * 60)
    
    # Paths
    original_numeric = "services/data/processed/diabetes_preprocessed_numeric.csv"
    enhanced_ml = "services/data/processed_enhanced/diabetes_enhanced_ml_ready.csv"
    original_readable = "services/data/processed/diabetes_preprocessed_readable.csv"
    enhanced_readable = "services/data/processed_enhanced/diabetes_enhanced_readable.csv"
    
    # Load datasets (if they exist)
    datasets = {}
    
    try:
        print("\nüìÇ Loading datasets...")
        
        if os.path.exists(enhanced_ml):
            enhanced_df = pd.read_csv(enhanced_ml)
            datasets['enhanced_ml'] = enhanced_df
            print(f"‚úÖ Enhanced ML dataset: {enhanced_df.shape}")
        
        if os.path.exists(enhanced_readable):
            enhanced_readable_df = pd.read_csv(enhanced_readable)
            datasets['enhanced_readable'] = enhanced_readable_df
            print(f"‚úÖ Enhanced readable dataset: {enhanced_readable_df.shape}")
            
        # Try to load original (might be too large)
        try:
            if os.path.exists(original_readable):
                # Just load a sample to check structure
                original_df = pd.read_csv(original_readable, nrows=1000)
                print(f"‚úÖ Original readable dataset sample: {original_df.shape}")
                datasets['original_sample'] = original_df
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not load original dataset: {str(e)}")
    
    except Exception as e:
        print(f"‚ùå Error loading datasets: {str(e)}")
        return
    
    # Analysis
    if 'enhanced_readable' in datasets:
        print(f"\nüìä ENHANCED DATASET ANALYSIS")
        print("-" * 40)
        
        df = datasets['enhanced_readable']
        
        # Basic info
        print(f"Dataset shape: {df.shape}")
        print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        
        # Column types
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
        
        print(f"\nColumn distribution:")
        print(f"- Numeric columns: {len(numeric_cols)}")
        print(f"- Categorical columns: {len(categorical_cols)}")
        
        # Check for year column handling
        if 'year' in categorical_cols:
            print("‚úÖ Year correctly handled as categorical")
            print(f"   Year values: {sorted(df['year'].unique())}")
        elif 'year' in numeric_cols:
            print("‚ùå Year still treated as numeric")
            
        # Check for new features
        new_features = ['bmi_risk_level', 'age_diabetes_risk', 'health_risk_score', 
                       'activity_score', 'sleep_quality', 'lifestyle_score', 'location_risk']
        
        found_features = [f for f in new_features if f in df.columns]
        print(f"\nüîß Engineered features found: {len(found_features)}")
        for feature in found_features:
            print(f"   - {feature}: {df[feature].nunique()} unique values")
        
        # Missing values
        missing = df.isnull().sum()
        total_missing = missing.sum()
        print(f"\nü©π Missing values: {total_missing} total")
        if total_missing > 0:
            print("   Columns with missing values:")
            for col, count in missing[missing > 0].items():
                print(f"   - {col}: {count} ({count/len(df)*100:.2f}%)")
        else:
            print("   ‚úÖ No missing values (imputation successful)")
            
    if 'enhanced_ml' in datasets:
        print(f"\nü§ñ ML-READY DATASET ANALYSIS")
        print("-" * 40)
        
        df_ml = datasets['enhanced_ml']
        print(f"ML dataset shape: {df_ml.shape}")
        
        # Check all columns are numeric (except target if present)
        non_numeric = df_ml.select_dtypes(exclude=[np.number]).columns.tolist()
        if 'diabetes' in non_numeric:
            non_numeric.remove('diabetes')
            
        if not non_numeric:
            print("‚úÖ All features properly encoded to numeric")
        else:
            print(f"‚ö†Ô∏è  Non-numeric columns found: {non_numeric}")
            
        # Check for scaled values (should be around mean=0, std=1 for scaled features)
        numeric_features = df_ml.select_dtypes(include=[np.number]).columns.tolist()
        if 'diabetes' in numeric_features:
            numeric_features.remove('diabetes')
            
        if numeric_features:
            means = df_ml[numeric_features].mean()
            stds = df_ml[numeric_features].std()
            
            # Check if values look scaled (mean close to 0, std close to 1)
            scaled_like = ((abs(means) < 0.1) & (abs(stds - 1) < 0.1)).sum()
            print(f"   Features that appear scaled: {scaled_like}/{len(numeric_features)}")
            
    # Feature selection analysis
    feature_selection_file = "services/data/processed_enhanced/feature_selection_report.csv"
    if os.path.exists(feature_selection_file):
        print(f"\nüéØ FEATURE SELECTION ANALYSIS")
        print("-" * 40)
        
        fs_df = pd.read_csv(feature_selection_file)
        selected_count = fs_df['selected'].sum()
        total_count = len(fs_df)
        
        print(f"Features analyzed: {total_count}")
        print(f"Features selected: {selected_count}")
        print(f"Reduction ratio: {(total_count - selected_count) / total_count * 100:.1f}%")
        
        print(f"\nTop 10 most important features:")
        top_features = fs_df.nlargest(10, 'score')
        for idx, row in top_features.iterrows():
            status = "‚úÖ" if row['selected'] else "‚ùå"
            print(f"   {status} {row['feature']}: {row['score']:.2f}")
    
    # Processing reports summary
    processing_summary_file = "services/data/processed_enhanced/processing_summary.csv"
    if os.path.exists(processing_summary_file):
        print(f"\nüìã PROCESSING SUMMARY")
        print("-" * 40)
        
        summary_df = pd.read_csv(processing_summary_file, index_col=0)
        for metric, value in summary_df['value'].items():
            print(f"   {metric}: {value}")
    
    print(f"\nüéâ ENHANCEMENT IMPACT SUMMARY")
    print("-" * 40)
    print("‚úÖ Year column: Fixed scaling issue (now categorical)")
    print("‚úÖ Feature engineering: Added 7 domain-specific features")
    print("‚úÖ Outlier handling: Implemented IQR-based capping")
    print("‚úÖ Advanced imputation: Medical domain-aware strategies")
    print("‚úÖ Feature selection: Reduced dimensionality intelligently")
    print("‚úÖ Comprehensive EDA: Full statistical and visual analysis")
    print("‚úÖ High cardinality handling: Grouped rare categories")
    print("‚úÖ Documentation: Complete processing traceability")
    
    print(f"\nüìÅ All enhanced files available in:")
    print(f"   - EDA Reports: services/data/reports_enhanced/")
    print(f"   - Processed Data: services/data/processed_enhanced/")

if __name__ == "__main__":
    load_and_compare_datasets()
