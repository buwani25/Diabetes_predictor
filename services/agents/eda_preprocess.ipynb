{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e2a99b",
   "metadata": {},
   "source": [
    "# Enhanced EDA & Preprocessing Pipeline for Diabetes Dataset\n",
    "\n",
    "This notebook converts the enhanced EDA and preprocessing pipeline for the diabetes dataset into an interactive format. It addresses gaps in year column handling, feature engineering, outlier detection, advanced imputation, comprehensive EDA, and high-dimensionality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b4086",
   "metadata": {},
   "source": [
    "# 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries including pandas, numpy, matplotlib, seaborn, sklearn, and others used in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8d4b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, TargetEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib backend for compatibility\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72970a68",
   "metadata": {},
   "source": [
    "# 2. Utility Functions\n",
    "\n",
    "Define utility functions for directory creation, binary detection, and target guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "199eb1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utility Functions ----------\n",
    "def ensure_dir(p: str):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def is_binary_like(s: pd.Series) -> bool:\n",
    "    \"\"\"Check if series contains binary-like values\"\"\"\n",
    "    vals = s.dropna().unique()\n",
    "    if len(vals) == 2:\n",
    "        return True\n",
    "    lowered = pd.Series(vals).astype(str).str.lower().unique()\n",
    "    return set(lowered).issubset({\"yes\",\"no\",\"true\",\"false\",\"positive\",\"negative\",\"pos\",\"neg\",\"y\",\"n\",\"1\",\"0\"})\n",
    "\n",
    "def guess_target(df: pd.DataFrame):\n",
    "    \"\"\"Automatically detect target column\"\"\"\n",
    "    common = [\n",
    "        \"Outcome\",\"outcome\",\"target\",\"Target\",\"label\",\"Label\",\"class\",\"Class\",\n",
    "        \"diabetes\",\"Diabetes\",\"has_diabetes\",\"diabetic\",\"Diabetic\"\n",
    "    ]\n",
    "    for c in common:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d475230",
   "metadata": {},
   "source": [
    "# 3. Enhanced EDA Functions\n",
    "\n",
    "Define functions for comprehensive exploratory data analysis including missing values analysis, correlation, and visualization generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f9403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Enhanced EDA Functions ----------\n",
    "def comprehensive_eda(df: pd.DataFrame, reports_dir: str, target_col: str = None):\n",
    "    \"\"\"Enhanced EDA with comprehensive analysis\"\"\"\n",
    "    ensure_dir(reports_dir)\n",
    "    \n",
    "    print(\"üîç Running Comprehensive EDA...\")\n",
    "    \n",
    "    # 1. Basic Dataset Info\n",
    "    basic_info = {\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'duplicated_rows': df.duplicated().sum()\n",
    "    }\n",
    "    \n",
    "    # 2. Column types and info\n",
    "    dtypes_df = df.dtypes.astype(str).rename(\"dtype\").reset_index().rename(columns={\"index\":\"column\"})\n",
    "    dtypes_df['unique_values'] = [df[col].nunique() for col in df.columns]\n",
    "    dtypes_df['null_count'] = [df[col].isnull().sum() for col in df.columns]\n",
    "    dtypes_df['null_percentage'] = dtypes_df['null_count'].apply(lambda x: round(x / len(df) * 100, 2))\n",
    "    dtypes_df.to_csv(os.path.join(reports_dir, \"01_enhanced_dtypes.csv\"), index=False)\n",
    "    \n",
    "    # 3. Enhanced missing values analysis with gender-specific context\n",
    "    miss_analysis = df.isnull().sum().reset_index()\n",
    "    miss_analysis.columns = ['column', 'missing_count']\n",
    "    miss_analysis['missing_pct'] = miss_analysis['missing_count'].apply(lambda x: round(x / len(df) * 100, 2))\n",
    "    \n",
    "    # Add context for gender-specific features\n",
    "    miss_analysis['missing_type'] = 'standard'\n",
    "    gender_specific_features = ['gestational_history', 'gestational_diabetes', 'pregnancy_history']\n",
    "    \n",
    "    for feature in gender_specific_features:\n",
    "        if feature in miss_analysis['column'].values:\n",
    "            mask = miss_analysis['column'] == feature\n",
    "            miss_analysis.loc[mask, 'missing_type'] = 'gender_specific'\n",
    "            \n",
    "            # If gender column exists, calculate male vs female missing rates\n",
    "            if 'gender' in df.columns or 'sex' in df.columns:\n",
    "                gender_col = 'gender' if 'gender' in df.columns else 'sex'\n",
    "                \n",
    "                # Calculate missing rates by gender\n",
    "                gender_stats = []\n",
    "                for gender_val in df[gender_col].unique():\n",
    "                    if pd.notna(gender_val):\n",
    "                        gender_subset = df[df[gender_col] == gender_val]\n",
    "                        gender_missing = gender_subset[feature].isnull().sum()\n",
    "                        gender_total = len(gender_subset)\n",
    "                        gender_pct = round(gender_missing / gender_total * 100, 2) if gender_total > 0 else 0\n",
    "                        gender_stats.append(f\"{gender_val}: {gender_missing}/{gender_total} ({gender_pct}%)\")\n",
    "                \n",
    "                # Add gender breakdown as a note\n",
    "                miss_analysis.loc[mask, 'gender_breakdown'] = \"; \".join(gender_stats)\n",
    "    \n",
    "    # Add gender_breakdown column for non-gender-specific features\n",
    "    if 'gender_breakdown' not in miss_analysis.columns:\n",
    "        miss_analysis['gender_breakdown'] = ''\n",
    "    miss_analysis['gender_breakdown'] = miss_analysis['gender_breakdown'].fillna('')\n",
    "    \n",
    "    miss_analysis = miss_analysis.sort_values('missing_pct', ascending=False)\n",
    "    miss_analysis.to_csv(os.path.join(reports_dir, \"02_enhanced_missing_values.csv\"), index=False)\n",
    "    \n",
    "    # 4. Identify column types\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Remove target from feature lists if specified\n",
    "    if target_col and target_col in numeric_cols:\n",
    "        numeric_cols.remove(target_col)\n",
    "    if target_col and target_col in categorical_cols:\n",
    "        categorical_cols.remove(target_col)\n",
    "    \n",
    "    # 5. Enhanced numeric analysis\n",
    "    if numeric_cols:\n",
    "        numeric_stats = df[numeric_cols].describe()\n",
    "        \n",
    "        # Add additional statistics\n",
    "        numeric_enhanced = numeric_stats.copy()\n",
    "        for col in numeric_cols:\n",
    "            data = df[col].dropna()\n",
    "            numeric_enhanced.loc['skewness', col] = stats.skew(data)\n",
    "            numeric_enhanced.loc['kurtosis', col] = stats.kurtosis(data)\n",
    "            numeric_enhanced.loc['cv', col] = data.std() / data.mean() if data.mean() != 0 else 0\n",
    "        \n",
    "        numeric_enhanced.round(4).to_csv(os.path.join(reports_dir, \"03_enhanced_numeric_analysis.csv\"))\n",
    "    \n",
    "    # 6. Categorical analysis\n",
    "    if categorical_cols:\n",
    "        cat_analysis = []\n",
    "        for col in categorical_cols:\n",
    "            unique_vals = df[col].nunique()\n",
    "            top_category = df[col].mode()[0] if not df[col].mode().empty else 'No Mode'\n",
    "            top_frequency = df[col].value_counts().iloc[0] if unique_vals > 0 else 0\n",
    "            \n",
    "            cat_analysis.append({\n",
    "                'column': col,\n",
    "                'unique_categories': unique_vals,\n",
    "                'top_category': top_category,\n",
    "                'top_frequency': top_frequency,\n",
    "                'top_percentage': round(top_frequency / len(df) * 100, 2)\n",
    "            })\n",
    "        \n",
    "        cat_df = pd.DataFrame(cat_analysis)\n",
    "        cat_df.to_csv(os.path.join(reports_dir, \"04_categorical_analysis.csv\"), index=False)\n",
    "    \n",
    "    # 7. Target distribution (if target specified)\n",
    "    if target_col and target_col in df.columns:\n",
    "        target_dist = df[target_col].value_counts().reset_index()\n",
    "        target_dist.columns = [target_col, 'count']\n",
    "        target_dist['percentage'] = target_dist['count'].apply(lambda x: round(x / len(df) * 100, 2))\n",
    "        target_dist.to_csv(os.path.join(reports_dir, \"05_target_distribution.csv\"), index=False)\n",
    "    \n",
    "    # 8. Outlier detection for numeric columns\n",
    "    outlier_analysis = []\n",
    "    for col in numeric_cols:\n",
    "        data = df[col].dropna()\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "        \n",
    "        outlier_analysis.append({\n",
    "            'column': col,\n",
    "            'outlier_count': len(outliers),\n",
    "            'outlier_percentage': round(len(outliers) / len(data) * 100, 2),\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound\n",
    "        })\n",
    "    \n",
    "    outlier_df = pd.DataFrame(outlier_analysis)\n",
    "    outlier_df.to_csv(os.path.join(reports_dir, \"06_outlier_analysis.csv\"), index=False)\n",
    "    \n",
    "    # 9. Correlation analysis (numeric columns only)\n",
    "    if len(numeric_cols) > 1:\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        \n",
    "        # Save correlation matrix\n",
    "        corr_matrix.round(3).to_csv(os.path.join(reports_dir, \"07_correlation_matrix.csv\"))\n",
    "        \n",
    "        # Create correlation heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                    square=True, linewidths=0.5)\n",
    "        plt.title('Feature Correlation Heatmap')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(reports_dir, \"correlation_heatmap.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 10. Generate comprehensive visualizations\n",
    "    generate_enhanced_visualizations(df, reports_dir, numeric_cols, categorical_cols, target_col)\n",
    "    \n",
    "    print(f\"‚úÖ Enhanced EDA completed. Reports saved to: {reports_dir}\")\n",
    "    return basic_info\n",
    "\n",
    "def generate_enhanced_visualizations(df, reports_dir, numeric_cols, categorical_cols, target_col):\n",
    "    \"\"\"Generate comprehensive visualizations for all columns\"\"\"\n",
    "    \n",
    "    # Create visualizations subdirectory\n",
    "    viz_dir = os.path.join(reports_dir, \"visualizations\")\n",
    "    ensure_dir(viz_dir)\n",
    "    \n",
    "    # 1. Numeric columns - Histograms and Box plots\n",
    "    for col in numeric_cols:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Histogram\n",
    "        df[col].hist(bins=50, ax=axes[0], alpha=0.7, edgecolor='black')\n",
    "        axes[0].set_title(f'Histogram: {col}')\n",
    "        axes[0].set_xlabel(col)\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        \n",
    "        # Box plot\n",
    "        df.boxplot(column=col, ax=axes[1])\n",
    "        axes[1].set_title(f'Box Plot: {col}')\n",
    "        axes[1].set_ylabel(col)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, f\"numeric_{col}.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. Categorical columns - Bar plots\n",
    "    for col in categorical_cols:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Get value counts\n",
    "        value_counts = df[col].value_counts()\n",
    "        \n",
    "        # Limit to top 20 categories if too many\n",
    "        if len(value_counts) > 20:\n",
    "            value_counts = value_counts.head(20)\n",
    "            title_suffix = \" (Top 20)\"\n",
    "        else:\n",
    "            title_suffix = \"\"\n",
    "        \n",
    "        # Create bar plot\n",
    "        ax = value_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "        plt.title(f'Distribution: {col}{title_suffix}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(value_counts.values):\n",
    "            ax.text(i, v + max(value_counts.values) * 0.01, str(v), \n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, f\"categorical_{col}.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 3. Target vs Features analysis (if target specified)\n",
    "    if target_col and target_col in df.columns:\n",
    "        target_viz_dir = os.path.join(viz_dir, \"target_analysis\")\n",
    "        ensure_dir(target_viz_dir)\n",
    "        \n",
    "        # Numeric features vs target\n",
    "        for col in numeric_cols:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # Create subplots\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # Box plot by target\n",
    "            df.boxplot(column=col, by=target_col, ax=axes[0])\n",
    "            axes[0].set_title(f'{col} by {target_col}')\n",
    "            \n",
    "            # Histogram by target\n",
    "            for target_val in df[target_col].unique():\n",
    "                subset = df[df[target_col] == target_val][col]\n",
    "                axes[1].hist(subset, alpha=0.7, label=f'{target_col}={target_val}', bins=30)\n",
    "            \n",
    "            axes[1].set_title(f'{col} Distribution by {target_col}')\n",
    "            axes[1].set_xlabel(col)\n",
    "            axes[1].set_ylabel('Frequency')\n",
    "            axes[1].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(target_viz_dir, f\"target_vs_{col}.png\"), dpi=300, bbox_inches='tight')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feeebdb",
   "metadata": {},
   "source": [
    "# 4. \n",
    "\n",
    "Define functions for outlier handling, imputation, feature engineering, and high cardinality categorical variable processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe0bad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Enhanced Preprocessing Functions ----------\n",
    "def detect_outliers_iqr(series, multiplier=1.5):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "def handle_outliers(df, numeric_cols, method='cap', multiplier=1.5):\n",
    "    \"\"\"Handle outliers in numeric columns\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    outlier_info = {}\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        outliers = detect_outliers_iqr(df_clean[col], multiplier)\n",
    "        outlier_count = outliers.sum()\n",
    "        \n",
    "        if outlier_count > 0:\n",
    "            if method == 'cap':\n",
    "                # Cap outliers\n",
    "                Q1 = df_clean[col].quantile(0.25)\n",
    "                Q3 = df_clean[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - multiplier * IQR\n",
    "                upper_bound = Q3 + multiplier * IQR\n",
    "                \n",
    "                df_clean[col] = np.where(df_clean[col] < lower_bound, lower_bound, df_clean[col])\n",
    "                df_clean[col] = np.where(df_clean[col] > upper_bound, upper_bound, df_clean[col])\n",
    "                \n",
    "            elif method == 'remove':\n",
    "                # Remove outliers (not recommended for large datasets)\n",
    "                df_clean = df_clean[~outliers]\n",
    "        \n",
    "        outlier_info[col] = {\n",
    "            'outlier_count': outlier_count,\n",
    "            'outlier_percentage': round(outlier_count / len(df) * 100, 2),\n",
    "            'method_applied': method if outlier_count > 0 else 'none'\n",
    "        }\n",
    "    \n",
    "    return df_clean, outlier_info\n",
    "\n",
    "def enhanced_imputation(df, numeric_cols, categorical_cols, target_col=None):\n",
    "    \"\"\"Enhanced imputation strategies for different column types\"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    imputation_info = {}\n",
    "    \n",
    "    # Medical/Health-specific imputation logic\n",
    "    medical_features = ['bmi', 'hbA1c_level', 'blood_glucose_level', 'sleep_hours']\n",
    "    \n",
    "    # Numeric imputation\n",
    "    for col in numeric_cols:\n",
    "        missing_count = df_imputed[col].isnull().sum()\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            if col in medical_features:\n",
    "                # For medical features, use median within similar groups if possible\n",
    "                if target_col and target_col in df.columns:\n",
    "                    # Group by target and use median\n",
    "                    df_imputed[col] = df_imputed.groupby(target_col)[col].transform(\n",
    "                        lambda x: x.fillna(x.median()) if not x.median() != x.median() else x.fillna(df_imputed[col].median())\n",
    "                    )\n",
    "                else:\n",
    "                    df_imputed[col] = df_imputed[col].fillna(df_imputed[col].median())\n",
    "                imputation_method = 'group_median' if target_col else 'median'\n",
    "            else:\n",
    "                # Regular median imputation for other numeric\n",
    "                df_imputed[col] = df_imputed[col].fillna(df_imputed[col].median())\n",
    "                imputation_method = 'median'\n",
    "            \n",
    "            imputation_info[col] = {\n",
    "                'missing_count': missing_count,\n",
    "                'imputation_method': imputation_method\n",
    "            }\n",
    "    \n",
    "    # Categorical imputation\n",
    "    for col in categorical_cols:\n",
    "        missing_count = df_imputed[col].isnull().sum()\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            # Special handling for gender-related features like gestational_history\n",
    "            if col.lower() in ['gestational_history', 'gestational_diabetes', 'pregnancy_history']:\n",
    "                # For gender-specific features, use gender-aware imputation\n",
    "                if 'gender' in df_imputed.columns or 'sex' in df_imputed.columns:\n",
    "                    gender_col = 'gender' if 'gender' in df_imputed.columns else 'sex'\n",
    "                    # For males, fill with 'Not Applicable' or 'NA'\n",
    "                    # For females, use mode within female group\n",
    "                    for gender_val in df_imputed[gender_col].unique():\n",
    "                        if pd.notna(gender_val):\n",
    "                            gender_mask = df_imputed[gender_col] == gender_val\n",
    "                            if gender_val.lower() in ['male', 'm', 'man']:\n",
    "                                df_imputed.loc[gender_mask, col] = df_imputed.loc[gender_mask, col].fillna('Not Applicable')\n",
    "                            else:  # female or other\n",
    "                                female_mode = df_imputed.loc[gender_mask, col].mode()\n",
    "                                if len(female_mode) > 0:\n",
    "                                    df_imputed.loc[gender_mask, col] = df_imputed.loc[gender_mask, col].fillna(female_mode[0])\n",
    "                                else:\n",
    "                                    df_imputed.loc[gender_mask, col] = df_imputed.loc[gender_mask, col].fillna('No')\n",
    "                    imputation_method = 'gender_aware'\n",
    "                else:\n",
    "                    # If no gender column, assume mixed population and use conservative approach\n",
    "                    df_imputed[col] = df_imputed[col].fillna('Not Applicable')\n",
    "                    imputation_method = 'not_applicable'\n",
    "            else:\n",
    "                # Regular categorical imputation for non-gender-specific features\n",
    "                # Use mode or 'Unknown' if no mode exists\n",
    "                mode_val = df_imputed[col].mode()\n",
    "                if len(mode_val) > 0:\n",
    "                    df_imputed[col] = df_imputed[col].fillna(mode_val[0])\n",
    "                    imputation_method = 'mode'\n",
    "                else:\n",
    "                    df_imputed[col] = df_imputed[col].fillna('Unknown')\n",
    "                    imputation_method = 'unknown'\n",
    "            \n",
    "            imputation_info[col] = {\n",
    "                'missing_count': missing_count,\n",
    "                'imputation_method': imputation_method\n",
    "            }\n",
    "    \n",
    "    return df_imputed, imputation_info\n",
    "\n",
    "def feature_engineering(df, target_col=None):\n",
    "    \"\"\"Create additional engineered features\"\"\"\n",
    "    df_engineered = df.copy()\n",
    "    new_features = []\n",
    "    \n",
    "    # 1. BMI-related features\n",
    "    if 'bmi' in df.columns:\n",
    "        # BMI risk categories (more detailed)\n",
    "        df_engineered['bmi_risk_level'] = pd.cut(df_engineered['bmi'], \n",
    "                                               bins=[0, 18.5, 25, 30, 35, float('inf')],\n",
    "                                               labels=['underweight', 'normal', 'overweight', 'obese_1', 'obese_2'])\n",
    "        new_features.append('bmi_risk_level')\n",
    "    \n",
    "    # 2. Age-related features\n",
    "    if 'age' in df.columns:\n",
    "        # Age risk for diabetes (medical domain knowledge)\n",
    "        df_engineered['age_diabetes_risk'] = pd.cut(df_engineered['age'],\n",
    "                                                  bins=[0, 35, 45, 65, float('inf')],\n",
    "                                                  labels=['low_risk', 'moderate_risk', 'high_risk', 'very_high_risk'])\n",
    "        new_features.append('age_diabetes_risk')\n",
    "    \n",
    "    # 3. Combined health risk score\n",
    "    health_indicators = ['hypertension', 'heart_disease', 'family_history']\n",
    "    available_indicators = [col for col in health_indicators if col in df.columns]\n",
    "    \n",
    "    if available_indicators:\n",
    "        df_engineered['health_risk_score'] = df_engineered[available_indicators].sum(axis=1)\n",
    "        new_features.append('health_risk_score')\n",
    "    \n",
    "    # 4. Lifestyle score\n",
    "    lifestyle_factors = []\n",
    "    \n",
    "    # Physical activity scoring\n",
    "    if 'physical_activity' in df.columns:\n",
    "        activity_map = {'low': 0, 'moderate': 1, 'high': 2}\n",
    "        df_engineered['activity_score'] = df_engineered['physical_activity'].map(activity_map).fillna(0)\n",
    "        lifestyle_factors.append('activity_score')\n",
    "        new_features.append('activity_score')\n",
    "    \n",
    "    # Sleep quality scoring\n",
    "    if 'sleep_hours' in df.columns:\n",
    "        # Optimal sleep is 7-9 hours\n",
    "        df_engineered['sleep_quality'] = df_engineered['sleep_hours'].apply(\n",
    "            lambda x: 2 if 7 <= x <= 9 else (1 if 6 <= x <= 10 else 0) if pd.notna(x) else 0\n",
    "        )\n",
    "        lifestyle_factors.append('sleep_quality')\n",
    "        new_features.append('sleep_quality')\n",
    "    \n",
    "    # Combined lifestyle score\n",
    "    if lifestyle_factors:\n",
    "        df_engineered['lifestyle_score'] = df_engineered[lifestyle_factors].sum(axis=1)\n",
    "        new_features.append('lifestyle_score')\n",
    "    \n",
    "    # 5. Geographic risk (if environmental_risk is available)\n",
    "    if 'environmental_risk' in df.columns and 'urban_rural' in df.columns:\n",
    "        # Combine environmental risk with urban/rural\n",
    "        urban_risk_map = {'urban': 1.1, 'rural': 0.9}  # Urban areas might have higher risk\n",
    "        df_engineered['location_risk'] = (df_engineered['environmental_risk'] * \n",
    "                                        df_engineered['urban_rural'].map(urban_risk_map).fillna(1.0))\n",
    "        new_features.append('location_risk')\n",
    "    \n",
    "    return df_engineered, new_features\n",
    "\n",
    "def handle_high_cardinality_categorical(df, categorical_cols, target_col=None, max_categories=10):\n",
    "    \"\"\"Handle high cardinality categorical variables\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    encoding_info = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        unique_count = df_processed[col].nunique()\n",
    "        \n",
    "        if unique_count > max_categories:\n",
    "            # For high cardinality columns like 'location' (states)\n",
    "            if col == 'location':\n",
    "                # Group by frequency - keep top states, others as 'Other'\n",
    "                value_counts = df_processed[col].value_counts()\n",
    "                top_categories = value_counts.head(max_categories).index.tolist()\n",
    "                df_processed[col] = df_processed[col].apply(\n",
    "                    lambda x: x if x in top_categories else 'Other'\n",
    "                )\n",
    "                encoding_info[col] = {\n",
    "                    'method': 'frequency_grouping',\n",
    "                    'kept_categories': len(top_categories) + 1,  # +1 for 'Other'\n",
    "                    'original_categories': unique_count\n",
    "                }\n",
    "            \n",
    "            elif target_col and target_col in df.columns:\n",
    "                # Use target encoding for other high cardinality categorical variables\n",
    "                # This is more sophisticated than frequency grouping\n",
    "                target_encoder = TargetEncoder()\n",
    "                df_processed[f'{col}_target_encoded'] = target_encoder.fit_transform(\n",
    "                    df_processed[[col]], df_processed[target_col]\n",
    "                )\n",
    "                \n",
    "                # Keep original column and add encoded version\n",
    "                encoding_info[col] = {\n",
    "                    'method': 'target_encoding',\n",
    "                    'new_column': f'{col}_target_encoded',\n",
    "                    'original_categories': unique_count\n",
    "                }\n",
    "            else:\n",
    "                # Fallback to frequency grouping\n",
    "                value_counts = df_processed[col].value_counts()\n",
    "                top_categories = value_counts.head(max_categories).index.tolist()\n",
    "                df_processed[col] = df_processed[col].apply(\n",
    "                    lambda x: x if x in top_categories else 'Other'\n",
    "                )\n",
    "                encoding_info[col] = {\n",
    "                    'method': 'frequency_grouping',\n",
    "                    'kept_categories': len(top_categories) + 1,\n",
    "                    'original_categories': unique_count\n",
    "                }\n",
    "    \n",
    "    return df_processed, encoding_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515849e2",
   "metadata": {},
   "source": [
    "# 5. Load Dataset\n",
    "\n",
    "Load the raw diabetes dataset from a CSV file and display basic information about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a35629f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 100000 rows √ó 28 columns\n",
      "üéØ Target column: diabetes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>location</th>\n",
       "      <th>race:AfricanAmerican</th>\n",
       "      <th>race:Asian</th>\n",
       "      <th>race:Caucasian</th>\n",
       "      <th>race:Hispanic</th>\n",
       "      <th>race:Other</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>...</th>\n",
       "      <th>diet_pattern</th>\n",
       "      <th>sleep_hours</th>\n",
       "      <th>alcohol_intake</th>\n",
       "      <th>family_history</th>\n",
       "      <th>medication_use</th>\n",
       "      <th>gestational_history</th>\n",
       "      <th>urban_rural</th>\n",
       "      <th>region_income</th>\n",
       "      <th>environmental_risk</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>Female</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>balanced</td>\n",
       "      <td>4</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>urban</td>\n",
       "      <td>low</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>Female</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>balanced</td>\n",
       "      <td>9</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>urban</td>\n",
       "      <td>medium</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>Male</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5</td>\n",
       "      <td>occasional</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urban</td>\n",
       "      <td>medium</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>Male</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>balanced</td>\n",
       "      <td>6</td>\n",
       "      <td>regular</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rural</td>\n",
       "      <td>low</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>Female</td>\n",
       "      <td>52.0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rural</td>\n",
       "      <td>medium</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  gender   age location  race:AfricanAmerican  race:Asian  \\\n",
       "0  2020  Female  32.0  Alabama                     0           0   \n",
       "1  2015  Female  29.0  Alabama                     0           1   \n",
       "2  2015    Male  18.0  Alabama                     0           0   \n",
       "3  2015    Male  41.0  Alabama                     0           0   \n",
       "4  2016  Female  52.0  Alabama                     1           0   \n",
       "\n",
       "   race:Caucasian  race:Hispanic  race:Other  hypertension  ...  diet_pattern  \\\n",
       "0               0              0           1             0  ...      balanced   \n",
       "1               0              0           0             0  ...      balanced   \n",
       "2               0              0           1             0  ...      balanced   \n",
       "3               1              0           0             0  ...      balanced   \n",
       "4               0              0           0             0  ...      balanced   \n",
       "\n",
       "  sleep_hours  alcohol_intake  family_history  medication_use  \\\n",
       "0           4            none               0               0   \n",
       "1           9            none               0               1   \n",
       "2           5      occasional               1               0   \n",
       "3           6         regular               1               0   \n",
       "4           5            none               0               0   \n",
       "\n",
       "  gestational_history urban_rural region_income environmental_risk  diabetes  \n",
       "0                 0.0       urban           low                  7         0  \n",
       "1                 1.0       urban        medium                  9         0  \n",
       "2                 NaN       urban        medium                 10         0  \n",
       "3                 NaN       rural           low                  2         0  \n",
       "4                 0.0       rural        medium                  6         0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "# Update the path below to the correct location of your diabetes dataset\n",
    "df = pd.read_csv(r'C:\\Users\\ASUS TUF A15\\Downloads\\diabetes_dataset_E.csv')\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "\n",
    "# Auto-detect target column\n",
    "target_col = guess_target(df)\n",
    "if target_col:\n",
    "    print(f\"üéØ Target column: {target_col}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No target column detected. Please specify manually.\")\n",
    "    target_col = None  # Set manually if needed\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70749b87",
   "metadata": {},
   "source": [
    "# 6. Identify Column Types\n",
    "\n",
    "Analyze and categorize columns into numeric and categorical types, with special handling for the year column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b420a770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 'year' from numeric to categorical (ordinal treatment)\n",
      "Numeric columns: ['age', 'race:AfricanAmerican', 'race:Asian', 'race:Caucasian', 'race:Hispanic', 'race:Other', 'hypertension', 'heart_disease', 'bmi', 'hbA1c_level', 'blood_glucose_level', 'sleep_hours', 'family_history', 'medication_use', 'gestational_history', 'environmental_risk']\n",
      "Categorical columns: ['gender', 'location', 'smoking_history', 'bmi_category', 'age_group', 'physical_activity', 'diet_pattern', 'alcohol_intake', 'urban_rural', 'region_income', 'year']\n"
     ]
    }
   ],
   "source": [
    "# Identify column types\n",
    "numeric_cols = [c for c in df.columns \n",
    "               if pd.api.types.is_numeric_dtype(df[c]) and c != target_col]\n",
    "categorical_cols = [c for c in df.columns \n",
    "                   if df[c].dtype == \"object\" and c != target_col]\n",
    "\n",
    "# Special handling for 'year' column - treat as categorical\n",
    "if 'year' in numeric_cols:\n",
    "    print(\"Moving 'year' from numeric to categorical (ordinal treatment)\")\n",
    "    numeric_cols.remove('year')\n",
    "    categorical_cols.append('year')\n",
    "    # Convert year to string to treat as categorical\n",
    "    df['year'] = df['year'].astype(str)\n",
    "\n",
    "print(f\"Numeric columns: {numeric_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa4e396",
   "metadata": {},
   "source": [
    "# 7. Run Comprehensive EDA\n",
    "\n",
    "Execute the comprehensive EDA function to generate reports, visualizations, and analyses on the dataset, including missing values, correlations, and outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75e4a463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Running Comprehensive EDA...\n",
      "‚úÖ Enhanced EDA completed. Reports saved to: data/reports_enhanced\n",
      "Basic info: {'total_rows': 100000, 'total_columns': 28, 'memory_usage_mb': np.float64(70.94676303863525), 'duplicated_rows': np.int64(0)}\n"
     ]
    }
   ],
   "source": [
    "# Run comprehensive EDA\n",
    "reports_dir = 'data/reports_enhanced'\n",
    "basic_info = comprehensive_eda(df, reports_dir=reports_dir, target_col=target_col)\n",
    "print(f\"Basic info: {basic_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016d392a",
   "metadata": {},
   "source": [
    "# 8. Handle Outliers\n",
    "\n",
    "Apply outlier detection and handling using the IQR method, capping or removing outliers as specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36647d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier handling completed.\n",
      "Outlier info: {'age': {'outlier_count': np.int64(0), 'outlier_percentage': np.float64(0.0), 'method_applied': 'none'}, 'race:AfricanAmerican': {'outlier_count': np.int64(20223), 'outlier_percentage': np.float64(20.22), 'method_applied': 'cap'}, 'race:Asian': {'outlier_count': np.int64(20015), 'outlier_percentage': np.float64(20.02), 'method_applied': 'cap'}, 'race:Caucasian': {'outlier_count': np.int64(19876), 'outlier_percentage': np.float64(19.88), 'method_applied': 'cap'}, 'race:Hispanic': {'outlier_count': np.int64(19888), 'outlier_percentage': np.float64(19.89), 'method_applied': 'cap'}, 'race:Other': {'outlier_count': np.int64(19998), 'outlier_percentage': np.float64(20.0), 'method_applied': 'cap'}, 'hypertension': {'outlier_count': np.int64(7485), 'outlier_percentage': np.float64(7.48), 'method_applied': 'cap'}, 'heart_disease': {'outlier_count': np.int64(3942), 'outlier_percentage': np.float64(3.94), 'method_applied': 'cap'}, 'bmi': {'outlier_count': np.int64(7086), 'outlier_percentage': np.float64(7.09), 'method_applied': 'cap'}, 'hbA1c_level': {'outlier_count': np.int64(1315), 'outlier_percentage': np.float64(1.32), 'method_applied': 'cap'}, 'blood_glucose_level': {'outlier_count': np.int64(2038), 'outlier_percentage': np.float64(2.04), 'method_applied': 'cap'}, 'sleep_hours': {'outlier_count': np.int64(0), 'outlier_percentage': np.float64(0.0), 'method_applied': 'none'}, 'family_history': {'outlier_count': np.int64(0), 'outlier_percentage': np.float64(0.0), 'method_applied': 'none'}, 'medication_use': {'outlier_count': np.int64(15045), 'outlier_percentage': np.float64(15.04), 'method_applied': 'cap'}, 'gestational_history': {'outlier_count': np.int64(5816), 'outlier_percentage': np.float64(5.82), 'method_applied': 'cap'}, 'environmental_risk': {'outlier_count': np.int64(0), 'outlier_percentage': np.float64(0.0), 'method_applied': 'none'}}\n"
     ]
    }
   ],
   "source": [
    "# Handle outliers\n",
    "df, outlier_info = handle_outliers(df, numeric_cols, method='cap')\n",
    "print(\"Outlier handling completed.\")\n",
    "print(f\"Outlier info: {outlier_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ab4b5a",
   "metadata": {},
   "source": [
    "# 9. Perform Enhanced Imputation\n",
    "\n",
    "Impute missing values in numeric and categorical columns using enhanced strategies, including group-based and gender-aware imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1384b01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation completed.\n",
      "Imputation info: {'gestational_history': {'missing_count': np.int64(41448), 'imputation_method': 'median'}}\n"
     ]
    }
   ],
   "source": [
    "# Perform enhanced imputation\n",
    "df, imputation_info = enhanced_imputation(df, numeric_cols, categorical_cols, target_col)\n",
    "print(\"Imputation completed.\")\n",
    "print(f\"Imputation info: {imputation_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2eca3",
   "metadata": {},
   "source": [
    "# 10. Apply Feature Engineering\n",
    "\n",
    "Create new features such as BMI risk levels, age diabetes risk, health risk scores, and lifestyle scores based on existing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c344452f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed. New features: ['bmi_risk_level', 'age_diabetes_risk', 'health_risk_score', 'activity_score', 'sleep_quality', 'lifestyle_score', 'location_risk']\n",
      "Updated numeric columns: ['age', 'race:AfricanAmerican', 'race:Asian', 'race:Caucasian', 'race:Hispanic', 'race:Other', 'hypertension', 'heart_disease', 'bmi', 'hbA1c_level', 'blood_glucose_level', 'sleep_hours', 'family_history', 'medication_use', 'gestational_history', 'environmental_risk', 'bmi_risk_level', 'age_diabetes_risk', 'health_risk_score', 'activity_score', 'sleep_quality', 'lifestyle_score', 'location_risk']\n",
      "Updated categorical columns: ['gender', 'location', 'smoking_history', 'bmi_category', 'age_group', 'physical_activity', 'diet_pattern', 'alcohol_intake', 'urban_rural', 'region_income', 'year']\n"
     ]
    }
   ],
   "source": [
    "# Apply feature engineering\n",
    "df, new_features = feature_engineering(df, target_col)\n",
    "print(f\"Feature engineering completed. New features: {new_features}\")\n",
    "\n",
    "# Update column lists with new categorical features\n",
    "new_categorical = [f for f in new_features if df[f].dtype == 'object']\n",
    "categorical_cols.extend(new_categorical)\n",
    "\n",
    "new_numeric = [f for f in new_features if f not in new_categorical]\n",
    "numeric_cols.extend(new_numeric)\n",
    "\n",
    "print(f\"Updated numeric columns: {numeric_cols}\")\n",
    "print(f\"Updated categorical columns: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4412bde2",
   "metadata": {},
   "source": [
    "# 11. Handle High Cardinality Categorical Variables\n",
    "\n",
    "Process high cardinality categorical columns using frequency grouping or target encoding to reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ddae2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High cardinality handling completed.\n",
      "Encoding info: {'location': {'method': 'frequency_grouping', 'kept_categories': 16, 'original_categories': 55}}\n"
     ]
    }
   ],
   "source": [
    "# Handle high cardinality categorical variables\n",
    "df, encoding_info = handle_high_cardinality_categorical(df, categorical_cols, target_col, max_categories=15)\n",
    "print(\"High cardinality handling completed.\")\n",
    "print(f\"Encoding info: {encoding_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950a56d",
   "metadata": {},
   "source": [
    "# 12. Prepare ML-Ready Data\n",
    "\n",
    "Perform one-hot encoding, scaling of numeric features, and optional feature selection to prepare the data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1729f592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying one-hot encoding to: ['gender', 'location', 'smoking_history', 'bmi_category', 'age_group', 'physical_activity', 'diet_pattern', 'alcohol_intake', 'urban_rural', 'region_income', 'year']\n",
      "Scaling 21 numeric features...\n",
      "Scaler saved to: data/processed_enhanced/feature_scaler.pkl\n",
      "Feature selection (too many features detected)...\n",
      "Selected 20 numeric features out of 75 (kept all 2 categorical features)\n",
      "Final dataset: 100000 rows √ó 23 columns\n"
     ]
    }
   ],
   "source": [
    "# Prepare ML-ready data\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "categorical_for_encoding = [col for col in categorical_cols \n",
    "                           if not any(f'{col}_target_encoded' in colname for colname in df.columns)]\n",
    "\n",
    "if categorical_for_encoding:\n",
    "    print(f\"Applying one-hot encoding to: {categorical_for_encoding}\")\n",
    "    df = pd.get_dummies(df, columns=categorical_for_encoding, drop_first=False)\n",
    "\n",
    "# Update numeric columns list (include target encoded features, exclude categorical features)\n",
    "target_encoded_cols = [col for col in df.columns if 'target_encoded' in col]\n",
    "\n",
    "# Filter numeric_cols to only include truly numeric columns that exist in df\n",
    "final_numeric_cols = []\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "        final_numeric_cols.append(col)\n",
    "\n",
    "# Add target encoded columns\n",
    "final_numeric_cols.extend(target_encoded_cols)\n",
    "\n",
    "# Scaling numeric features (excluding year which is now categorical)\n",
    "if final_numeric_cols:\n",
    "    print(f\"Scaling {len(final_numeric_cols)} numeric features...\")\n",
    "    scaler = StandardScaler()\n",
    "    df[final_numeric_cols] = scaler.fit_transform(df[final_numeric_cols])\n",
    "    \n",
    "    # Save scaler for later use\n",
    "    import joblib\n",
    "    scaler_path = 'data/processed_enhanced/feature_scaler.pkl'\n",
    "    ensure_dir('data/processed_enhanced')\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Feature selection (optional - select top K features for numeric columns only)\n",
    "if target_col and target_col in df.columns and len(df.columns) > 50:\n",
    "    print(\"Feature selection (too many features detected)...\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Only apply feature selection to numeric columns\n",
    "    numeric_feature_cols = [col for col in X.columns if pd.api.types.is_numeric_dtype(X[col])]\n",
    "    categorical_feature_cols = [col for col in X.columns if not pd.api.types.is_numeric_dtype(X[col])]\n",
    "    \n",
    "    if numeric_feature_cols and len(numeric_feature_cols) > 30:\n",
    "        # Select top K numeric features\n",
    "        k = min(20, len(numeric_feature_cols))  # Select top 20 numeric or all available\n",
    "        selector = SelectKBest(score_func=f_classif, k=k)\n",
    "        X_numeric_selected = selector.fit_transform(X[numeric_feature_cols], y)\n",
    "        \n",
    "        # Get selected feature names\n",
    "        selected_numeric_features = pd.Series(numeric_feature_cols)[selector.get_support()].tolist()\n",
    "        \n",
    "        # Combine selected numeric features with all categorical features and target\n",
    "        selected_features = selected_numeric_features + categorical_feature_cols + [target_col]\n",
    "        \n",
    "        df = df[selected_features]\n",
    "        \n",
    "        # Save feature selection info\n",
    "        feature_scores = pd.DataFrame({\n",
    "            'feature': numeric_feature_cols,\n",
    "            'score': selector.scores_,\n",
    "            'selected': selector.get_support()\n",
    "        }).sort_values('score', ascending=False)\n",
    "        \n",
    "        feature_scores.to_csv('data/processed_enhanced/feature_selection_report.csv', index=False)\n",
    "        print(f\"Selected {k} numeric features out of {len(numeric_feature_cols)} (kept all {len(categorical_feature_cols)} categorical features)\")\n",
    "    else:\n",
    "        print(\"Skipping feature selection - not enough numeric features or features already manageable\")\n",
    "\n",
    "print(f\"Final dataset: {df.shape[0]} rows √ó {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fed788",
   "metadata": {},
   "source": [
    "# 13. Save Processed Data\n",
    "\n",
    "Save the human-readable and ML-ready versions of the processed dataset to CSV files, along with reports and summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c2cc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed data saved!\n",
      "üìÅ Human-readable data: data/processed_enhanced/diabetes_enhanced_readable.csv\n",
      "ü§ñ ML-ready data: data/processed_enhanced/diabetes_enhanced_ml_ready.csv\n",
      "üìä Final dataset: 100000 rows √ó 23 columns\n",
      "üìã EDA reports available in: data/reports_enhanced\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "\n",
    "# Ensure output directory exists\n",
    "ensure_dir('data/processed_enhanced')\n",
    "\n",
    "# Save human-readable version\n",
    "readable_path = 'data/processed_enhanced/diabetes_enhanced_readable.csv'\n",
    "df.to_csv(readable_path, index=False)\n",
    "\n",
    "# Save ML-ready version (same as readable in this case, since we did the transformations in place)\n",
    "ml_path = 'data/processed_enhanced/diabetes_enhanced_ml_ready.csv'\n",
    "df.to_csv(ml_path, index=False)\n",
    "\n",
    "print(\"‚úÖ Processed data saved!\")\n",
    "print(f\"üìÅ Human-readable data: {readable_path}\")\n",
    "print(f\"ü§ñ ML-ready data: {ml_path}\")\n",
    "print(f\"üìä Final dataset: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"üìã EDA reports available in: {reports_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
